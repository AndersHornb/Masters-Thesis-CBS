import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import re
import warnings

from sklearn.base import BaseEstimator, TransformerMixin
from sklearn.compose import ColumnTransformer, make_column_selector as selector
from sklearn.preprocessing import StandardScaler, OneHotEncoder, FunctionTransformer
from sklearn.impute import SimpleImputer
from sklearn.pipeline import Pipeline
from sklearn.model_selection import train_test_split, RepeatedStratifiedKFold, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (
    classification_report, roc_curve, roc_auc_score,
    precision_recall_curve, average_precision_score,
    brier_score_loss, balanced_accuracy_score,
    matthews_corrcoef, cohen_kappa_score
)
from sklearn.calibration import calibration_curve

# suppress benign warnings
warnings.filterwarnings("ignore", message="Found unknown categories", category=UserWarning)
warnings.filterwarnings("ignore", message="Skipping features without any observed values", category=UserWarning)

# 1. Load Data
def load_data(path):
    df = pd.read_excel("MLModels.xlsx", sheet_name="Sheet1 (2)")
    y = df["Underpriced"]
    X = df.drop(columns=["Underpriced"]).copy()
    X = X.dropna(axis=1, how='all').loc[:, ~X.columns.str.startswith('Unnamed')]
    return X, y

# 2. Feature‐engineering helpers
numeric_cols = [
    'Offer Price\n(USD)\n(σ=Median)',
    'Proceeds\nAmount This\nMarket\n(USD, Millions)\n(σ=Com)',
    'Financials:\nTotal\nRevenues\nBefore\nOffering\n(USD,\nMillions)\n(σ=Median)',
    'Financials:\nNet Income\nAfter Taxes\nBefore\nOffering\n(USD,\nMillions)\n(σ=Median)',
    '15 day return'
]
enh_feats = ['OfferPrice_log','Proceeds_log','ProfitMargin','MarketBull']

def feature_engineer(df, medians=None):
    df = df.copy()
    for c in numeric_cols:
        df[c] = pd.to_numeric(df.get(c, np.nan), errors='coerce')
    df['OfferPrice_log'] = np.log1p(df[numeric_cols[0]].fillna(0))
    df['Proceeds_log']   = np.log1p(df[numeric_cols[1]].fillna(0))
    rev = df[numeric_cols[2]].replace(0, np.nan)
    ni  = df[numeric_cols[3]].fillna(0)
    df['ProfitMargin']   = ni / rev
    df['MarketBull']     = (df[numeric_cols[4]] > 0).astype(int)

    fe_df = df[enh_feats].replace([np.inf, -np.inf], np.nan)
    if medians is None:
        medians = fe_df.median()
    df[enh_feats] = fe_df.fillna(medians)
    return df, medians

class FeatureEngineer(BaseEstimator, TransformerMixin):
    def __init__(self, drop_patterns=None):
        self.drop_patterns = drop_patterns or []
        self.raw_to_drop = numeric_cols.copy()
        self.drop_patterns += [
            'Issuer/Borrower', 'Ticker.*Symbol',
            'Book Or Co-Managers', 'Stock Price', 'Percent'
        ]
        self.medians = None
        self.feature_cols = None
        self.num_cols = None

    def fit(self, X, y=None):
        X2, self.medians = feature_engineer(X)
        for pat in self.drop_patterns:
            cols = [c for c in X2.columns if re.search(pat, c, re.IGNORECASE)]
            X2.drop(columns=cols, inplace=True, errors='ignore')
        X2.drop(columns=self.raw_to_drop, inplace=True, errors='ignore')
        self.feature_cols = X2.columns.tolist()
        self.num_cols = X2.select_dtypes(include=[np.number]).columns.tolist()
        return self

    def transform(self, X):
        X2, _ = feature_engineer(X, medians=self.medians)
        for pat in self.drop_patterns:
            cols = [c for c in X2.columns if re.search(pat, c, re.IGNORECASE)]
            X2.drop(columns=cols, inplace=True, errors='ignore')
        X2.drop(columns=self.raw_to_drop, inplace=True, errors='ignore')
        X2 = X2.reindex(columns=self.feature_cols, fill_value=np.nan)
        for c in self.num_cols:
            X2[c] = pd.to_numeric(X2[c], errors='coerce')
        return X2

class CategoryCleaner(BaseEstimator, TransformerMixin):
    def fit(self, X, y=None):
        self.categories_ = {col: set(X[col].dropna().unique()) for col in X.columns}
        return self
    def transform(self, X):
        X2 = X.copy()
        for col, cats in self.categories_.items():
            mask = ~X2[col].isin(cats)
            X2.loc[mask, col] = np.nan
        return X2

# 3. Preprocessing
preprocessor = ColumnTransformer([
    ('num', Pipeline([
        ('imp',   SimpleImputer(strategy='median')),
        ('scale', StandardScaler())
    ]), selector(dtype_include=np.number)),
    ('cat', Pipeline([
        ('clean', CategoryCleaner()),
        ('imp',   SimpleImputer(strategy='constant', fill_value='Unknown')),
        ('ohe',   OneHotEncoder(
                      drop='first',
                      handle_unknown='infrequent_if_exist',
                      min_frequency=1,
                      sparse_output=False))
    ]), selector(dtype_include=object))
], remainder='drop')

# dense transformer for OHE
to_dense = FunctionTransformer(
    lambda X: X.toarray() if hasattr(X, 'toarray') else X,
    validate=False
)

# 4. Load & split
X, y = load_data("MLModels.xlsx")
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, stratify=y, random_state=42
)

# 5. Pipeline with pure LogisticRegression
pipeline = Pipeline([
    ('fe',    FeatureEngineer()),
    ('prep',  preprocessor),
    ('dense', to_dense),
    ('clf',   LogisticRegression(
                   penalty='l2',
                   C=1.0,
                   solver='liblinear',
                   max_iter=1000,
                   class_weight='balanced',
                   n_jobs=-1,
                   random_state=42
               ))
])

# 6. Robust cross‐validated performance estimate
cv = RepeatedStratifiedKFold(n_splits=5, n_repeats=3, random_state=42)
cv_scores = cross_val_score(
    pipeline, X, y,
    scoring='balanced_accuracy',
    cv=cv,
    n_jobs=-1
)

from sklearn.model_selection import ShuffleSplit

# 6b. Monte-Carlo hold-out distribution
shuff = ShuffleSplit(n_splits=30, test_size=0.2, random_state=42)
test_scores = cross_val_score(
    pipeline,     # your LogisticRegression pipeline
    X, y,
    scoring='balanced_accuracy',
    cv=shuff,
    n_jobs=-1
)
print(f"ShuffleSplit hold-out mean±std: {test_scores.mean():.3f} ± {test_scores.std():.3f}")


print(f"Cross‐validated balanced accuracy: {cv_scores.mean():.3f} ± {cv_scores.std():.3f}")

# 7. Final fit & evaluation on hold‐out
print("\nFitting on full training set and evaluating on test set…")
pipeline.fit(X_train, y_train)

y_pred  = pipeline.predict(X_test)
y_proba = pipeline.predict_proba(X_test)[:, 1]

print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
plt.figure(); 
plt.plot(fpr, tpr, label=f"AUC = {roc_auc_score(y_test, y_proba):.3f}")
plt.plot([0,1],[0,1],'k--'); plt.xlabel("FPR"); plt.ylabel("TPR")
plt.title("ROC Curve"); plt.legend(); plt.show()

# Precision‐Recall Curve
prec, rec, _ = precision_recall_curve(y_test, y_proba)
ap = average_precision_score(y_test, y_proba)
plt.figure(); 
plt.step(rec, prec, where='post')
plt.xlabel("Recall"); plt.ylabel("Precision")
plt.title(f"Precision-Recall (AP = {ap:.3f})"); plt.show()

# Calibration & Brier
frac_pos, mean_pred = calibration_curve(y_test, y_proba, n_bins=10)
plt.figure(); 
plt.plot(mean_pred, frac_pos, marker='o')
plt.plot([0,1],[0,1],'k--')
plt.xlabel("Mean predicted"); plt.ylabel("Fraction positive")
plt.title("Calibration Curve"); plt.show()

print(f"\nBrier score:       {brier_score_loss(y_test, y_proba):.4f}")
print(f"Balanced accuracy: {balanced_accuracy_score(y_test, y_pred):.4f}")
print(f"Matthews corrcoef: {matthews_corrcoef(y_test, y_pred):.4f}")
print(f"Cohen's kappa:     {cohen_kappa_score(y_test, y_pred):.4f}")

# 8. Feature variance diagnostics
fe = pipeline.named_steps['fe']
X_tr_feats = fe.transform(X_train)
X_te_feats = fe.transform(X_test)
num_feats = X_tr_feats.select_dtypes(include=[np.number]).columns
var_df = pd.DataFrame({
    'train_variance': X_tr_feats[num_feats].var(),
    'test_variance':  X_te_feats[num_feats].var()
})
print("\nFeature variances (train vs test):")
print(var_df)
var_df.to_csv("feature_variances.csv", index=True)
print("Saved feature_variances.csv")
